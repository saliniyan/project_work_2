{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sNZMfpCbGBGu"
      },
      "outputs": [],
      "source": [
        "#Libraries\n",
        "import pathlib\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import callbacks\n",
        "from keras.saving import register_keras_serializable\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@register_keras_serializable()\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)"
      ],
      "metadata": {
        "id": "yqM0fdc2GFv3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@register_keras_serializable()\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "         return keras.ops.not_equal(inputs, 0)"
      ],
      "metadata": {
        "id": "C8eHSOpxGLHz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@register_keras_serializable()\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(latent_dim, activation=\"relu\"),\n",
        "                layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        # Causal self-attention (decoder side)\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask,\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        # Cross-attention (NO causal/padding mask to avoid shape mismatch)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, seq_len = input_shape[0], input_shape[1]\n",
        "        i = tf.range(seq_len)[:, tf.newaxis]\n",
        "        j = tf.range(seq_len)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, seq_len, seq_len))\n",
        "        return tf.tile(mask, [batch_size, 1, 1])\n"
      ],
      "metadata": {
        "id": "ee2EcRWpGNRm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@register_keras_serializable()\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "  def get_config(self):\n",
        "    return {\"d_model\": self.d_model.numpy(), \"warmup_steps\": self.warmup_steps}"
      ],
      "metadata": {
        "id": "2hXuiUiJGOp-"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.models import load_model\n",
        "from google.colab import drive\n",
        "import json, os\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "MODEL_DIR = \"/content/drive/MyDrive/model/spanish_to_english\"\n",
        "sequence_length = 30  # MUST match training\n",
        "\n",
        "# Load model\n",
        "transformer = load_model(os.path.join(MODEL_DIR, \"spanish.keras\"))\n",
        "\n",
        "# Load vocab\n",
        "with open(os.path.join(MODEL_DIR, \"eng_vocab.json\")) as f:\n",
        "    eng_vocab = json.load(f)\n",
        "\n",
        "with open(os.path.join(MODEL_DIR, \"es_vocab.json\")) as f:\n",
        "    es_vocab = json.load(f)\n",
        "\n",
        "# Recreate vectorizers\n",
        "es_vectorization = TextVectorization(\n",
        "    vocabulary=es_vocab,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length\n",
        ")\n",
        "\n",
        "eng_vectorization = TextVectorization(\n",
        "    vocabulary=eng_vocab,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length\n",
        ")\n",
        "\n",
        "# END token id\n",
        "end_token_id = eng_vocab.index(\"end\")\n",
        "\n",
        "# Translation function\n",
        "def translate_spanish(spanish_text, max_len=30):\n",
        "    encoder_input = es_vectorization([spanish_text])\n",
        "    decoder_input = tf.zeros((1, 1), dtype=tf.int64)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        preds = transformer(\n",
        "            {\n",
        "                \"encoder_inputs\": encoder_input,\n",
        "                \"decoder_inputs\": decoder_input,\n",
        "            },\n",
        "            training=False\n",
        "        )\n",
        "\n",
        "        next_token = tf.argmax(preds[:, -1, :], axis=-1)\n",
        "        token_id = int(next_token[0])\n",
        "\n",
        "        if token_id == end_token_id:\n",
        "            break\n",
        "\n",
        "        decoder_input = tf.concat(\n",
        "            [decoder_input, tf.expand_dims(next_token, axis=-1)],\n",
        "            axis=-1\n",
        "        )\n",
        "\n",
        "    words = [\n",
        "        eng_vocab[t]\n",
        "        for t in decoder_input.numpy()[0]\n",
        "        if t < len(eng_vocab) and eng_vocab[t] not in (\"\", \"end\")\n",
        "    ]\n",
        "\n",
        "    return \" \".join(words).strip()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvOY5qFvI4ap",
        "outputId": "a328399a-6b9a-4885-94b2-c313e4157d08"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'positional_embedding', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'transformer_encoder', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'positional_embedding_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/layer.py:421: UserWarning: `build()` was called on layer 'transformer_decoder', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(translate_spanish(\"ve.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xm6Pd2x7f23l",
        "outputId": "3d8e984a-a54b-4871-d47e-f0d69e9e17ae"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "go\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.models import load_model\n",
        "from google.colab import drive\n",
        "import json, os\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "MODEL_DIR = \"/content/drive/MyDrive/hindi_to_english\"\n",
        "sequence_length = 30  # MUST match training\n",
        "\n",
        "# Load model\n",
        "transformer = load_model(os.path.join(MODEL_DIR, \"model.keras\"))\n",
        "\n",
        "# Load vocab\n",
        "with open(os.path.join(MODEL_DIR, \"eng_vocab.json\")) as f:\n",
        "    eng_vocab = json.load(f)\n",
        "\n",
        "with open(os.path.join(MODEL_DIR, \"hi_vocab.json\")) as f:\n",
        "    hi_vocab = json.load(f)\n",
        "\n",
        "# Recreate vectorizers\n",
        "hi_vectorization = TextVectorization(\n",
        "    vocabulary=hi_vocab,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length\n",
        ")\n",
        "\n",
        "eng_vectorization = TextVectorization(\n",
        "    vocabulary=eng_vocab,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length\n",
        ")\n",
        "\n",
        "# END token id\n",
        "end_token_id = eng_vocab.index(\"end\")\n",
        "\n",
        "# Translation function\n",
        "def translate_hindi(hindi_text, max_len=30):\n",
        "    encoder_input = hi_vectorization([hindi_text])\n",
        "    decoder_input = tf.zeros((1, 1), dtype=tf.int64)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        preds = transformer(\n",
        "            {\n",
        "                \"encoder_inputs\": encoder_input,\n",
        "                \"decoder_inputs\": decoder_input,\n",
        "            },\n",
        "            training=False\n",
        "        )\n",
        "\n",
        "        next_token = tf.argmax(preds[:, -1, :], axis=-1)\n",
        "        token_id = int(next_token[0])\n",
        "\n",
        "        if token_id == end_token_id:\n",
        "            break\n",
        "\n",
        "        decoder_input = tf.concat(\n",
        "            [decoder_input, tf.expand_dims(next_token, axis=-1)],\n",
        "            axis=-1\n",
        "        )\n",
        "\n",
        "    words = [\n",
        "        eng_vocab[t]\n",
        "        for t in decoder_input.numpy()[0]\n",
        "        if t < len(eng_vocab) and eng_vocab[t] not in (\"\", \"end\")\n",
        "    ]\n",
        "\n",
        "    return \" \".join(words).strip()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPaUMqLiJyJE",
        "outputId": "2e2eddd9-6934-4572-81bf-a60d1f6cb3a2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(translate_hindi(\"आप कैसे हैं\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpenRkLrf-Ze",
        "outputId": "41897d8d-8717-48c1-f218-40d2bf3fbe16"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "how are you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.models import load_model\n",
        "from google.colab import drive\n",
        "import json, os\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "MODEL_DIR = \"/content/drive/MyDrive/model/malayalam_to_english\"\n",
        "sequence_length = 30  # MUST match training\n",
        "\n",
        "# Load model\n",
        "transformer = load_model(os.path.join(MODEL_DIR, \"model.keras\"))\n",
        "\n",
        "# Load vocab\n",
        "with open(os.path.join(MODEL_DIR, \"eng_vocab.json\")) as f:\n",
        "    eng_vocab = json.load(f)\n",
        "\n",
        "with open(os.path.join(MODEL_DIR, \"ml_vocab.json\")) as f:\n",
        "    ml_vocab = json.load(f)\n",
        "\n",
        "# Recreate vectorizers\n",
        "ml_vectorization = TextVectorization(\n",
        "    vocabulary=ml_vocab,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length\n",
        ")\n",
        "\n",
        "eng_vectorization = TextVectorization(\n",
        "    vocabulary=eng_vocab,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length\n",
        ")\n",
        "\n",
        "# END token id\n",
        "end_token_id = eng_vocab.index(\"end\")\n",
        "\n",
        "# Translation function\n",
        "def translate_malayalam(malayalam_text, max_len=30):\n",
        "    encoder_input = ml_vectorization([malayalam_text])\n",
        "    decoder_input = tf.zeros((1, 1), dtype=tf.int64)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        preds = transformer(\n",
        "            {\n",
        "                \"encoder_inputs\": encoder_input,\n",
        "                \"decoder_inputs\": decoder_input,\n",
        "            },\n",
        "            training=False\n",
        "        )\n",
        "\n",
        "        next_token = tf.argmax(preds[:, -1, :], axis=-1)\n",
        "        token_id = int(next_token[0])\n",
        "\n",
        "        if token_id == end_token_id:\n",
        "            break\n",
        "\n",
        "        decoder_input = tf.concat(\n",
        "            [decoder_input, tf.expand_dims(next_token, axis=-1)],\n",
        "            axis=-1\n",
        "        )\n",
        "\n",
        "    words = [\n",
        "        eng_vocab[t]\n",
        "        for t in decoder_input.numpy()[0]\n",
        "        if t < len(eng_vocab) and eng_vocab[t] not in (\"\", \"end\")\n",
        "    ]\n",
        "\n",
        "    return \" \".join(words).strip()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ET1rHsFMQbDQ",
        "outputId": "d1b622ab-a065-4c03-b63c-71591b02e2e0"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(translate_malayalam(\"മാധ്യമങ്ങള്‍ക്ക് നിയന്ത്രണം [UNK] ഹരജി ഹൈക്കോടതി തള്ളി\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDlTTLFegFWF",
        "outputId": "59ac9e1a-af97-479a-c84b-a6ee61335cb4"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the rejected plea plea filed by hc order\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from tensorflow.keras.models import load_model\n",
        "from google.colab import drive\n",
        "import json, os\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount(\"/content/drive\", force_remount=False)\n",
        "\n",
        "MODEL_DIR = \"/content/drive/MyDrive/model/eng_to_tamil\"\n",
        "sequence_length = 30  # MUST match training\n",
        "\n",
        "# Load model\n",
        "transformer = load_model(os.path.join(MODEL_DIR, \"model.keras\"))\n",
        "\n",
        "# Load vocab\n",
        "with open(os.path.join(MODEL_DIR, \"en_vocab.json\")) as f:\n",
        "    en_vocab = json.load(f)\n",
        "\n",
        "with open(os.path.join(MODEL_DIR, \"ta_vocab.json\")) as f:\n",
        "    ta_vocab = json.load(f)\n",
        "\n",
        "# Recreate vectorizers\n",
        "en_vectorization = TextVectorization(\n",
        "    vocabulary=en_vocab,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length\n",
        ")\n",
        "\n",
        "ta_vectorization = TextVectorization(\n",
        "    vocabulary=ta_vocab,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length\n",
        ")\n",
        "\n",
        "# END token id (from Tamil vocab)\n",
        "end_token_id = ta_vocab.index(\"[end]\")\n",
        "\n",
        "# Translation function\n",
        "def translate_tamil(english_text, max_len=30):\n",
        "    encoder_input = en_vectorization([english_text])\n",
        "    decoder_input = tf.zeros((1, 1), dtype=tf.int64)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        preds = transformer(\n",
        "            {\n",
        "                \"encoder_inputs\": encoder_input,\n",
        "                \"decoder_inputs\": decoder_input,\n",
        "            },\n",
        "            training=False\n",
        "        )\n",
        "\n",
        "        next_token = tf.argmax(preds[:, -1, :], axis=-1)\n",
        "        token_id = int(next_token[0])\n",
        "\n",
        "        # Stop when END token appears\n",
        "        if token_id == end_token_id:\n",
        "            break\n",
        "\n",
        "        decoder_input = tf.concat(\n",
        "            [decoder_input, tf.expand_dims(next_token, axis=-1)],\n",
        "            axis=-1\n",
        "        )\n",
        "\n",
        "    words = [\n",
        "        ta_vocab[t]\n",
        "        for t in decoder_input.numpy()[0]\n",
        "        if t < len(ta_vocab) and ta_vocab[t] not in (\"\", \"end\")\n",
        "    ]\n",
        "\n",
        "    return \" \".join(words).strip()\n"
      ],
      "metadata": {
        "id": "02gR4MypTLnm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b738e2b9-e093-4541-890b-8215c61bb665"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(translate_tamil(\"how are you\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJ_pUpGbad-n",
        "outputId": "e1afc389-4553-4c0a-b0d9-0ff8b6144451"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "நீங்கள் எப்படி இருக்கிறீர்கள்\n"
          ]
        }
      ]
    }
  ]
}